{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohanrajmit/Transfer-Learning/blob/master/Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdxVC0hnBMo3"
      },
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Empty lists to hold the images and labels for each each image\n",
        "x_train = []\n",
        "y_train = []"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmEmDeXKHtyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68e82f6-20dd-4c5e-b012-153016638196"
      },
      "source": [
        "!git clone https://github.com/mohanrajmit/Transfer-Learning.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Transfer-Learning'...\n",
            "remote: Enumerating objects: 2244, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 2244 (delta 12), reused 0 (delta 0), pack-reused 2223\u001b[K\n",
            "Receiving objects: 100% (2244/2244), 13.88 MiB | 14.24 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Es9qfMKV5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3f52c5-7ccd-4d61-9298-3b93a097b81f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bird_feature_classifier_model_vgg.h5  sample_data  Transfer-Learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFzZzArPBQW6"
      },
      "source": [
        "# Load the training data set by looping over every image file\n",
        "for image_file in Path(\"Transfer-Learning/training_dataset\").glob(\"**/*.png\"):\n",
        "\n",
        "    # Load the current image file\n",
        "    image_data = load_img(image_file, target_size=(224, 224))\n",
        "\n",
        "    # Convert the loaded image file to a numpy array\n",
        "    image_array = img_to_array(image_data)\n",
        "\n",
        "    # Add the current image to our list of training images\n",
        "    x_train.append(image_array)\n",
        "\n",
        "    # Add a label for this image. If it was a not_bird image, label it 0. If it was a bird, label it 1.\n",
        "    if \"not_bird\" in image_file.stem:\n",
        "        y_train.append(0)\n",
        "    else:\n",
        "        y_train.append(1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(x_train).shape)"
      ],
      "metadata": {
        "id": "3VKExNyOP9_J",
        "outputId": "2dfdb24a-eebb-426d-929a-66c92e43c8ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBfVYK56BbwT"
      },
      "source": [
        "# Convert the list of separate images into a single 4D numpy array. This is what Keras expects.\n",
        "x_train = np.array(x_train)\n",
        "\n",
        "# Normalize image data to 0-to-1 range\n",
        "x_train = imagenet_utils.preprocess_input(x_train)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the pre-trained neural network to use as a feature extractor\n",
        "feature_extractor = VGG16(weights='imagenet', include_top=False,input_shape=(224,224,3))\n",
        "\n"
      ],
      "metadata": {
        "id": "4JHK86iUQjMy",
        "outputId": "9bb3d6c3-0bfb-42cd-ab7f-f72e02788b63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyOa6dvE3vEh",
        "outputId": "7f0dc084-f653-46fb-80b4-b4b3944a0f6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_resnet = ResNet50(weights='imagenet', include_top=False,input_shape=(224,224,3))"
      ],
      "metadata": {
        "id": "a3AWHb_c3XtD",
        "outputId": "17936478-3bed-47f0-810b-69b4776ecd05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_resnet.summary()"
      ],
      "metadata": {
        "id": "lIcMxUqd3jd1",
        "outputId": "dbcd2d04-03b8-469e-bf2a-a37ccd240656",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 230, 230, 3)          0         ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 112, 112, 64)         9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalizati  (None, 112, 112, 64)         256       ['conv1_conv[0][0]']          \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)     (None, 112, 112, 64)         0         ['conv1_bn[0][0]']            \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 114, 114, 64)         0         ['conv1_relu[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 56, 56, 64)           0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 56, 56, 64)           4160      ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 56, 56, 256)          16640     ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)      (None, 56, 56, 256)          0         ['conv2_block1_0_bn[0][0]',   \n",
            "                                                                     'conv2_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activati  (None, 56, 56, 256)          0         ['conv2_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 56, 56, 64)           16448     ['conv2_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)      (None, 56, 56, 256)          0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activati  (None, 56, 56, 256)          0         ['conv2_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 56, 56, 64)           16448     ['conv2_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)      (None, 56, 56, 256)          0         ['conv2_block2_out[0][0]',    \n",
            "                                                                     'conv2_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activati  (None, 56, 56, 256)          0         ['conv2_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 28, 28, 128)          32896     ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 28, 28, 512)          131584    ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)      (None, 28, 28, 512)          0         ['conv3_block1_0_bn[0][0]',   \n",
            "                                                                     'conv3_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activati  (None, 28, 28, 512)          0         ['conv3_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)      (None, 28, 28, 512)          0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activati  (None, 28, 28, 512)          0         ['conv3_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)      (None, 28, 28, 512)          0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activati  (None, 28, 28, 512)          0         ['conv3_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)      (None, 28, 28, 512)          0         ['conv3_block3_out[0][0]',    \n",
            "                                                                     'conv3_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activati  (None, 28, 28, 512)          0         ['conv3_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 14, 14, 256)          131328    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 14, 14, 1024)         525312    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_0_bn[0][0]',   \n",
            "                                                                     'conv4_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block5_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block5_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block5_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block5_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block6_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block6_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block6_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block5_out[0][0]',    \n",
            "                                                                     'conv4_block6_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block6_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2  (None, 7, 7, 512)            524800    ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2  (None, 7, 7, 2048)           2099200   ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_0_bn[0][0]',   \n",
            "                                                                     'conv5_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2  (None, 7, 7, 512)            1049088   ['conv5_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_out[0][0]',    \n",
            "                                                                     'conv5_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2  (None, 7, 7, 512)            1049088   ['conv5_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block2_out[0][0]',    \n",
            "                                                                     'conv5_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23587712 (89.98 MB)\n",
            "Trainable params: 23534592 (89.78 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features for each image (all in one pass)\n",
        "features_x = feature_extractor.predict(x_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZaqOt889QnPX",
        "outputId": "f7d837f7-df9b-4794-e737-70015557d5b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 8s 128ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_x.shape"
      ],
      "metadata": {
        "id": "nJ14eKvORWjc",
        "outputId": "39dd9d11-f1d3-4813-c5e9-2ba9a79fd27f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 7, 7, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_resnet = feature_extractor_resnet.predict(x_train)"
      ],
      "metadata": {
        "id": "z6gAkGeh3uQ5",
        "outputId": "617621cb-d058-48a6-e7d3-a46b91fe6d52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 12s 123ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_resnet.shape"
      ],
      "metadata": {
        "id": "xDhUQYLl3yd0",
        "outputId": "54b1f466-ae71-439d-8fce-05ae22a8b003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 7, 7, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w7PoomPsYHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4686b983-f9d0-482b-dbe8-c2988d2e2dc4"
      },
      "source": [
        "Y_train = np.array(y_train)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvvFDEMpKwQt"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn81_QfVBvuj"
      },
      "source": [
        "# Create a model_vgg and add layers\n",
        "model_vgg = Sequential()\n",
        "\n",
        "# Add layers to our model_vgg\n",
        "model_vgg.add(Flatten(input_shape=features_x.shape[1:]))\n",
        "model_vgg.add(Dense(128, activation='relu'))\n",
        "model_vgg.add(Dropout(0.5))\n",
        "model_vgg.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model_vgg\n",
        "model_vgg.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVPx-aid5gnB",
        "outputId": "bcdc2fd3-ce72-4b53-a973-f3bfee83fc42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               3211392   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3211521 (12.25 MB)\n",
            "Trainable params: 3211521 (12.25 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model_vgg and add layers\n",
        "model_resnet = Sequential()\n",
        "\n",
        "# Add layers to our model_vgg\n",
        "model_resnet.add(Flatten(input_shape=features_resnet.shape[1:]))\n",
        "model_resnet.add(Dense(128, activation='relu'))\n",
        "model_resnet.add(Dropout(0.5))\n",
        "model_resnet.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model_vgg\n",
        "model_resnet.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "YYQK1YgD4XmP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model_vgg\n",
        "model_resnet.fit(features_resnet,Y_train,epochs=200, verbose=1)"
      ],
      "metadata": {
        "id": "wIdRSkp1GTgT",
        "outputId": "3f5c86b6-70cb-437e-d683-7ad268282654",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "63/63 [==============================] - 2s 12ms/step - loss: 3.3761 - accuracy: 0.7520\n",
            "Epoch 2/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.3206 - accuracy: 0.8485\n",
            "Epoch 3/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.2515 - accuracy: 0.8830\n",
            "Epoch 4/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.1883 - accuracy: 0.9165\n",
            "Epoch 5/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.1358 - accuracy: 0.9355\n",
            "Epoch 6/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.1195 - accuracy: 0.9505\n",
            "Epoch 7/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.1231 - accuracy: 0.9500\n",
            "Epoch 8/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.1000 - accuracy: 0.9495\n",
            "Epoch 9/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.1087 - accuracy: 0.9500\n",
            "Epoch 10/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0882 - accuracy: 0.9595\n",
            "Epoch 11/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0811 - accuracy: 0.9630\n",
            "Epoch 12/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0814 - accuracy: 0.9620\n",
            "Epoch 13/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.1106 - accuracy: 0.9545\n",
            "Epoch 14/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0823 - accuracy: 0.9655\n",
            "Epoch 15/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0744 - accuracy: 0.9695\n",
            "Epoch 16/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0663 - accuracy: 0.9700\n",
            "Epoch 17/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0879 - accuracy: 0.9665\n",
            "Epoch 18/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0660 - accuracy: 0.9725\n",
            "Epoch 19/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0738 - accuracy: 0.9715\n",
            "Epoch 20/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0991 - accuracy: 0.9695\n",
            "Epoch 21/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0671 - accuracy: 0.9715\n",
            "Epoch 22/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0661 - accuracy: 0.9695\n",
            "Epoch 23/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0620 - accuracy: 0.9780\n",
            "Epoch 24/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0581 - accuracy: 0.9740\n",
            "Epoch 25/200\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.0676 - accuracy: 0.9640\n",
            "Epoch 26/200\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0661 - accuracy: 0.9730\n",
            "Epoch 27/200\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0651 - accuracy: 0.9735\n",
            "Epoch 28/200\n",
            "63/63 [==============================] - 1s 21ms/step - loss: 0.0860 - accuracy: 0.9685\n",
            "Epoch 29/200\n",
            "63/63 [==============================] - 2s 31ms/step - loss: 0.0629 - accuracy: 0.9750\n",
            "Epoch 30/200\n",
            "63/63 [==============================] - 2s 25ms/step - loss: 0.0747 - accuracy: 0.9740\n",
            "Epoch 31/200\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0722 - accuracy: 0.9775\n",
            "Epoch 32/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0459 - accuracy: 0.9780\n",
            "Epoch 33/200\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0510 - accuracy: 0.9755\n",
            "Epoch 34/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0360 - accuracy: 0.9855\n",
            "Epoch 35/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0401 - accuracy: 0.9820\n",
            "Epoch 36/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0801 - accuracy: 0.9670\n",
            "Epoch 37/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0569 - accuracy: 0.9745\n",
            "Epoch 38/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0387 - accuracy: 0.9815\n",
            "Epoch 39/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0449 - accuracy: 0.9765\n",
            "Epoch 40/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0557 - accuracy: 0.9775\n",
            "Epoch 41/200\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 0.0520 - accuracy: 0.9795\n",
            "Epoch 42/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0434 - accuracy: 0.9780\n",
            "Epoch 43/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0455 - accuracy: 0.9825\n",
            "Epoch 44/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0333 - accuracy: 0.9835\n",
            "Epoch 45/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0343 - accuracy: 0.9865\n",
            "Epoch 46/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0375 - accuracy: 0.9810\n",
            "Epoch 47/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0339 - accuracy: 0.9870\n",
            "Epoch 48/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0302 - accuracy: 0.9890\n",
            "Epoch 49/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0572 - accuracy: 0.9810\n",
            "Epoch 50/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0291 - accuracy: 0.9860\n",
            "Epoch 51/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0409 - accuracy: 0.9790\n",
            "Epoch 52/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0452 - accuracy: 0.9815\n",
            "Epoch 53/200\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0289 - accuracy: 0.9850\n",
            "Epoch 54/200\n",
            "63/63 [==============================] - 1s 21ms/step - loss: 0.0312 - accuracy: 0.9835\n",
            "Epoch 55/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0391 - accuracy: 0.9830\n",
            "Epoch 56/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0391 - accuracy: 0.9840\n",
            "Epoch 57/200\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.0529 - accuracy: 0.9795\n",
            "Epoch 58/200\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.0351 - accuracy: 0.9835\n",
            "Epoch 59/200\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0552 - accuracy: 0.9790\n",
            "Epoch 60/200\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.0479 - accuracy: 0.9800\n",
            "Epoch 61/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0353 - accuracy: 0.9820\n",
            "Epoch 62/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0380 - accuracy: 0.9810\n",
            "Epoch 63/200\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - accuracy: 0.9840\n",
            "Epoch 64/200\n",
            "63/63 [==============================] - 1s 23ms/step - loss: 0.0408 - accuracy: 0.9795\n",
            "Epoch 65/200\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.0334 - accuracy: 0.9840\n",
            "Epoch 66/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0269 - accuracy: 0.9835\n",
            "Epoch 67/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0355 - accuracy: 0.9855\n",
            "Epoch 68/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0426 - accuracy: 0.9805\n",
            "Epoch 69/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0296 - accuracy: 0.9865\n",
            "Epoch 70/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0662 - accuracy: 0.9800\n",
            "Epoch 71/200\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0437 - accuracy: 0.9805\n",
            "Epoch 72/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0458 - accuracy: 0.9795\n",
            "Epoch 73/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0724 - accuracy: 0.9670\n",
            "Epoch 74/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0646 - accuracy: 0.9685\n",
            "Epoch 75/200\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 0.0513 - accuracy: 0.9810\n",
            "Epoch 76/200\n",
            "63/63 [==============================] - 1s 21ms/step - loss: 0.0400 - accuracy: 0.9785\n",
            "Epoch 77/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0537 - accuracy: 0.9720\n",
            "Epoch 78/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0506 - accuracy: 0.9710\n",
            "Epoch 79/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0434 - accuracy: 0.9715\n",
            "Epoch 80/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0410 - accuracy: 0.9730\n",
            "Epoch 81/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0578 - accuracy: 0.9710\n",
            "Epoch 82/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0330 - accuracy: 0.9800\n",
            "Epoch 83/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0254 - accuracy: 0.9850\n",
            "Epoch 84/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0341 - accuracy: 0.9845\n",
            "Epoch 85/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0336 - accuracy: 0.9815\n",
            "Epoch 86/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0372 - accuracy: 0.9815\n",
            "Epoch 87/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0420 - accuracy: 0.9785\n",
            "Epoch 88/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0484 - accuracy: 0.9820\n",
            "Epoch 89/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0429 - accuracy: 0.9785\n",
            "Epoch 90/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0487 - accuracy: 0.9795\n",
            "Epoch 91/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0332 - accuracy: 0.9825\n",
            "Epoch 92/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0375 - accuracy: 0.9840\n",
            "Epoch 93/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0316 - accuracy: 0.9840\n",
            "Epoch 94/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0298 - accuracy: 0.9780\n",
            "Epoch 95/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0300 - accuracy: 0.9820\n",
            "Epoch 96/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0410 - accuracy: 0.9825\n",
            "Epoch 97/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0391 - accuracy: 0.9745\n",
            "Epoch 98/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0323 - accuracy: 0.9855\n",
            "Epoch 99/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0316 - accuracy: 0.9820\n",
            "Epoch 100/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0538 - accuracy: 0.9780\n",
            "Epoch 101/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0561 - accuracy: 0.9720\n",
            "Epoch 102/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0414 - accuracy: 0.9765\n",
            "Epoch 103/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0430 - accuracy: 0.9765\n",
            "Epoch 104/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0456 - accuracy: 0.9790\n",
            "Epoch 105/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0418 - accuracy: 0.9835\n",
            "Epoch 106/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0224 - accuracy: 0.9845\n",
            "Epoch 107/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0368 - accuracy: 0.9800\n",
            "Epoch 108/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0353 - accuracy: 0.9825\n",
            "Epoch 109/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0212 - accuracy: 0.9880\n",
            "Epoch 110/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0298 - accuracy: 0.9905\n",
            "Epoch 111/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0525 - accuracy: 0.9810\n",
            "Epoch 112/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0463 - accuracy: 0.9795\n",
            "Epoch 113/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0464 - accuracy: 0.9810\n",
            "Epoch 114/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0263 - accuracy: 0.9840\n",
            "Epoch 115/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0248 - accuracy: 0.9850\n",
            "Epoch 116/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0394 - accuracy: 0.9785\n",
            "Epoch 117/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0264 - accuracy: 0.9830\n",
            "Epoch 118/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0357 - accuracy: 0.9850\n",
            "Epoch 119/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0260 - accuracy: 0.9890\n",
            "Epoch 120/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0257 - accuracy: 0.9850\n",
            "Epoch 121/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0279 - accuracy: 0.9825\n",
            "Epoch 122/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0213 - accuracy: 0.9860\n",
            "Epoch 123/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0331 - accuracy: 0.9860\n",
            "Epoch 124/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0296 - accuracy: 0.9895\n",
            "Epoch 125/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0252 - accuracy: 0.9875\n",
            "Epoch 126/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0376 - accuracy: 0.9835\n",
            "Epoch 127/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0375 - accuracy: 0.9795\n",
            "Epoch 128/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0420 - accuracy: 0.9815\n",
            "Epoch 129/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0345 - accuracy: 0.9800\n",
            "Epoch 130/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0334 - accuracy: 0.9890\n",
            "Epoch 131/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0328 - accuracy: 0.9905\n",
            "Epoch 132/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0295 - accuracy: 0.9895\n",
            "Epoch 133/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0231 - accuracy: 0.9905\n",
            "Epoch 134/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0249 - accuracy: 0.9870\n",
            "Epoch 135/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0197 - accuracy: 0.9910\n",
            "Epoch 136/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0257 - accuracy: 0.9905\n",
            "Epoch 137/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0252 - accuracy: 0.9890\n",
            "Epoch 138/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0221 - accuracy: 0.9905\n",
            "Epoch 139/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0246 - accuracy: 0.9890\n",
            "Epoch 140/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0283 - accuracy: 0.9875\n",
            "Epoch 141/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0293 - accuracy: 0.9850\n",
            "Epoch 142/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0211 - accuracy: 0.9910\n",
            "Epoch 143/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0176 - accuracy: 0.9945\n",
            "Epoch 144/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0244 - accuracy: 0.9875\n",
            "Epoch 145/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0153 - accuracy: 0.9920\n",
            "Epoch 146/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0300 - accuracy: 0.9860\n",
            "Epoch 147/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0223 - accuracy: 0.9920\n",
            "Epoch 148/200\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.0165 - accuracy: 0.9915\n",
            "Epoch 149/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0227 - accuracy: 0.9920\n",
            "Epoch 150/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0263 - accuracy: 0.9870\n",
            "Epoch 151/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0268 - accuracy: 0.9870\n",
            "Epoch 152/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0256 - accuracy: 0.9895\n",
            "Epoch 153/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0300 - accuracy: 0.9850\n",
            "Epoch 154/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0345 - accuracy: 0.9855\n",
            "Epoch 155/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0257 - accuracy: 0.9885\n",
            "Epoch 156/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0188 - accuracy: 0.9935\n",
            "Epoch 157/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0189 - accuracy: 0.9895\n",
            "Epoch 158/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0227 - accuracy: 0.9895\n",
            "Epoch 159/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0257 - accuracy: 0.9870\n",
            "Epoch 160/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0456 - accuracy: 0.9825\n",
            "Epoch 161/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0249 - accuracy: 0.9885\n",
            "Epoch 162/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0248 - accuracy: 0.9865\n",
            "Epoch 163/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0167 - accuracy: 0.9925\n",
            "Epoch 164/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0331 - accuracy: 0.9850\n",
            "Epoch 165/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0172 - accuracy: 0.9925\n",
            "Epoch 166/200\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.0490 - accuracy: 0.9870\n",
            "Epoch 167/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0413 - accuracy: 0.9845\n",
            "Epoch 168/200\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.0281 - accuracy: 0.9840\n",
            "Epoch 169/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0356 - accuracy: 0.9890\n",
            "Epoch 170/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0520 - accuracy: 0.9870\n",
            "Epoch 171/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0261 - accuracy: 0.9845\n",
            "Epoch 172/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0272 - accuracy: 0.9880\n",
            "Epoch 173/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0221 - accuracy: 0.9925\n",
            "Epoch 174/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0139 - accuracy: 0.9935\n",
            "Epoch 175/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0397 - accuracy: 0.9875\n",
            "Epoch 176/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0310 - accuracy: 0.9925\n",
            "Epoch 177/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0379 - accuracy: 0.9885\n",
            "Epoch 178/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0384 - accuracy: 0.9905\n",
            "Epoch 179/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0365 - accuracy: 0.9880\n",
            "Epoch 180/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0197 - accuracy: 0.9955\n",
            "Epoch 181/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0330 - accuracy: 0.9845\n",
            "Epoch 182/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0318 - accuracy: 0.9865\n",
            "Epoch 183/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0156 - accuracy: 0.9920\n",
            "Epoch 184/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0306 - accuracy: 0.9865\n",
            "Epoch 185/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0288 - accuracy: 0.9890\n",
            "Epoch 186/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0150 - accuracy: 0.9915\n",
            "Epoch 187/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0113 - accuracy: 0.9925\n",
            "Epoch 188/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0164 - accuracy: 0.9925\n",
            "Epoch 189/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0290 - accuracy: 0.9875\n",
            "Epoch 190/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0344 - accuracy: 0.9865\n",
            "Epoch 191/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0322 - accuracy: 0.9840\n",
            "Epoch 192/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0346 - accuracy: 0.9830\n",
            "Epoch 193/200\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.0166 - accuracy: 0.9900\n",
            "Epoch 194/200\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.0217 - accuracy: 0.9890\n",
            "Epoch 195/200\n",
            "63/63 [==============================] - 2s 26ms/step - loss: 0.0201 - accuracy: 0.9930\n",
            "Epoch 196/200\n",
            "63/63 [==============================] - 2s 27ms/step - loss: 0.0262 - accuracy: 0.9910\n",
            "Epoch 197/200\n",
            "63/63 [==============================] - 1s 23ms/step - loss: 0.0132 - accuracy: 0.9935\n",
            "Epoch 198/200\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0217 - accuracy: 0.9920\n",
            "Epoch 199/200\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.0295 - accuracy: 0.9920\n",
            "Epoch 200/200\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.0187 - accuracy: 0.9925\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79d58e5fec20>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rxngy6-Bynu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986fa557-aaeb-4b34-e81a-14174e188ffa"
      },
      "source": [
        "# Train the model_vgg\n",
        "model_vgg.fit(features_x,Y_train,epochs=200, verbose=1)\n",
        "\n",
        "# Save the trained model_vgg to a file so we can use it to make predictions later\n",
        "model_vgg.save(\"bird_feature_classifier_model_vgg.h5\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.1069 - accuracy: 0.9520\n",
            "Epoch 2/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0938 - accuracy: 0.9610\n",
            "Epoch 3/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0917 - accuracy: 0.9560\n",
            "Epoch 4/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0949 - accuracy: 0.9610\n",
            "Epoch 5/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0841 - accuracy: 0.9675\n",
            "Epoch 6/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0712 - accuracy: 0.9700\n",
            "Epoch 7/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0728 - accuracy: 0.9630\n",
            "Epoch 8/200\n",
            "63/63 [==============================] - 1s 8ms/step - loss: 0.0761 - accuracy: 0.9635\n",
            "Epoch 9/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0654 - accuracy: 0.9670\n",
            "Epoch 10/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0750 - accuracy: 0.9675\n",
            "Epoch 11/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 0.9690\n",
            "Epoch 12/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0704 - accuracy: 0.9635\n",
            "Epoch 13/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0639 - accuracy: 0.9750\n",
            "Epoch 14/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0573 - accuracy: 0.9710\n",
            "Epoch 15/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0659 - accuracy: 0.9655\n",
            "Epoch 16/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0453 - accuracy: 0.9765\n",
            "Epoch 17/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0566 - accuracy: 0.9745\n",
            "Epoch 18/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0512 - accuracy: 0.9725\n",
            "Epoch 19/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0395 - accuracy: 0.9790\n",
            "Epoch 20/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0721 - accuracy: 0.9690\n",
            "Epoch 21/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9715\n",
            "Epoch 22/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0633 - accuracy: 0.9710\n",
            "Epoch 23/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0596 - accuracy: 0.9725\n",
            "Epoch 24/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.9775\n",
            "Epoch 25/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0481 - accuracy: 0.9795\n",
            "Epoch 26/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0667 - accuracy: 0.9745\n",
            "Epoch 27/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0591 - accuracy: 0.9755\n",
            "Epoch 28/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0505 - accuracy: 0.9750\n",
            "Epoch 29/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0798 - accuracy: 0.9660\n",
            "Epoch 30/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0639 - accuracy: 0.9700\n",
            "Epoch 31/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0483 - accuracy: 0.9760\n",
            "Epoch 32/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0395 - accuracy: 0.9790\n",
            "Epoch 33/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0485 - accuracy: 0.9800\n",
            "Epoch 34/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0583 - accuracy: 0.9750\n",
            "Epoch 35/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9710\n",
            "Epoch 36/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0623 - accuracy: 0.9755\n",
            "Epoch 37/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 0.9725\n",
            "Epoch 38/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.9815\n",
            "Epoch 39/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0630 - accuracy: 0.9725\n",
            "Epoch 40/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9720\n",
            "Epoch 41/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9815\n",
            "Epoch 42/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.9800\n",
            "Epoch 43/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.9765\n",
            "Epoch 44/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0654 - accuracy: 0.9815\n",
            "Epoch 45/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0361 - accuracy: 0.9805\n",
            "Epoch 46/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0351 - accuracy: 0.9840\n",
            "Epoch 47/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0293 - accuracy: 0.9870\n",
            "Epoch 48/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0435 - accuracy: 0.9810\n",
            "Epoch 49/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0474 - accuracy: 0.9800\n",
            "Epoch 50/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0362 - accuracy: 0.9795\n",
            "Epoch 51/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0441 - accuracy: 0.9790\n",
            "Epoch 52/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0301 - accuracy: 0.9820\n",
            "Epoch 53/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 0.9880\n",
            "Epoch 54/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0328 - accuracy: 0.9840\n",
            "Epoch 55/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0561 - accuracy: 0.9800\n",
            "Epoch 56/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0355 - accuracy: 0.9820\n",
            "Epoch 57/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 0.9855\n",
            "Epoch 58/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0302 - accuracy: 0.9865\n",
            "Epoch 59/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0277 - accuracy: 0.9855\n",
            "Epoch 60/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 0.9800\n",
            "Epoch 61/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9845\n",
            "Epoch 62/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 0.9865\n",
            "Epoch 63/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.9810\n",
            "Epoch 64/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.9860\n",
            "Epoch 65/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0279 - accuracy: 0.9860\n",
            "Epoch 66/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.9865\n",
            "Epoch 67/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0417 - accuracy: 0.9800\n",
            "Epoch 68/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0339 - accuracy: 0.9875\n",
            "Epoch 69/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 0.9880\n",
            "Epoch 70/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9830\n",
            "Epoch 71/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0232 - accuracy: 0.9890\n",
            "Epoch 72/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0181 - accuracy: 0.9905\n",
            "Epoch 73/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9830\n",
            "Epoch 74/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0400 - accuracy: 0.9825\n",
            "Epoch 75/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0361 - accuracy: 0.9845\n",
            "Epoch 76/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 0.9855\n",
            "Epoch 77/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0315 - accuracy: 0.9845\n",
            "Epoch 78/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0338 - accuracy: 0.9820\n",
            "Epoch 79/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0395 - accuracy: 0.9840\n",
            "Epoch 80/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0400 - accuracy: 0.9860\n",
            "Epoch 81/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.9845\n",
            "Epoch 82/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.9880\n",
            "Epoch 83/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.9865\n",
            "Epoch 84/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0367 - accuracy: 0.9825\n",
            "Epoch 85/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0343 - accuracy: 0.9860\n",
            "Epoch 86/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.9815\n",
            "Epoch 87/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0233 - accuracy: 0.9855\n",
            "Epoch 88/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0358 - accuracy: 0.9825\n",
            "Epoch 89/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0536 - accuracy: 0.9825\n",
            "Epoch 90/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 0.9915\n",
            "Epoch 91/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 0.9855\n",
            "Epoch 92/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0364 - accuracy: 0.9855\n",
            "Epoch 93/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0312 - accuracy: 0.9800\n",
            "Epoch 94/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9900\n",
            "Epoch 95/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0402 - accuracy: 0.9835\n",
            "Epoch 96/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0257 - accuracy: 0.9885\n",
            "Epoch 97/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 0.9825\n",
            "Epoch 98/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.9785\n",
            "Epoch 99/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0331 - accuracy: 0.9855\n",
            "Epoch 100/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 0.9815\n",
            "Epoch 101/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0266 - accuracy: 0.9890\n",
            "Epoch 102/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 0.9875\n",
            "Epoch 103/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 0.9860\n",
            "Epoch 104/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0280 - accuracy: 0.9890\n",
            "Epoch 105/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0302 - accuracy: 0.9820\n",
            "Epoch 106/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0310 - accuracy: 0.9845\n",
            "Epoch 107/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.9875\n",
            "Epoch 108/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0235 - accuracy: 0.9890\n",
            "Epoch 109/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0314 - accuracy: 0.9890\n",
            "Epoch 110/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 0.9915\n",
            "Epoch 111/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 0.9915\n",
            "Epoch 112/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0310 - accuracy: 0.9880\n",
            "Epoch 113/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9900\n",
            "Epoch 114/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0292 - accuracy: 0.9885\n",
            "Epoch 115/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 0.9855\n",
            "Epoch 116/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 0.9915\n",
            "Epoch 117/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 0.9865\n",
            "Epoch 118/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0465 - accuracy: 0.9860\n",
            "Epoch 119/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0243 - accuracy: 0.9825\n",
            "Epoch 120/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0273 - accuracy: 0.9850\n",
            "Epoch 121/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0212 - accuracy: 0.9875\n",
            "Epoch 122/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0210 - accuracy: 0.9900\n",
            "Epoch 123/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0165 - accuracy: 0.9925\n",
            "Epoch 124/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0189 - accuracy: 0.9895\n",
            "Epoch 125/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 0.9880\n",
            "Epoch 126/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 0.9870\n",
            "Epoch 127/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9925\n",
            "Epoch 128/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0232 - accuracy: 0.9885\n",
            "Epoch 129/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0212 - accuracy: 0.9900\n",
            "Epoch 130/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 0.9915\n",
            "Epoch 131/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 0.9890\n",
            "Epoch 132/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0258 - accuracy: 0.9880\n",
            "Epoch 133/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 0.9880\n",
            "Epoch 134/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 0.9875\n",
            "Epoch 135/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9910\n",
            "Epoch 136/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0221 - accuracy: 0.9905\n",
            "Epoch 137/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0259 - accuracy: 0.9890\n",
            "Epoch 138/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9875\n",
            "Epoch 139/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0228 - accuracy: 0.9880\n",
            "Epoch 140/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0175 - accuracy: 0.9930\n",
            "Epoch 141/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0258 - accuracy: 0.9925\n",
            "Epoch 142/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 0.9870\n",
            "Epoch 143/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0145 - accuracy: 0.9935\n",
            "Epoch 144/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0247 - accuracy: 0.9880\n",
            "Epoch 145/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.9885\n",
            "Epoch 146/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 0.9870\n",
            "Epoch 147/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0293 - accuracy: 0.9840\n",
            "Epoch 148/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 0.9860\n",
            "Epoch 149/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0237 - accuracy: 0.9890\n",
            "Epoch 150/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 0.9920\n",
            "Epoch 151/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0362 - accuracy: 0.9855\n",
            "Epoch 152/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0391 - accuracy: 0.9905\n",
            "Epoch 153/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0148 - accuracy: 0.9930\n",
            "Epoch 154/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0174 - accuracy: 0.9890\n",
            "Epoch 155/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0177 - accuracy: 0.9930\n",
            "Epoch 156/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0239 - accuracy: 0.9890\n",
            "Epoch 157/200\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 0.9895\n",
            "Epoch 158/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0186 - accuracy: 0.9930\n",
            "Epoch 159/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0322 - accuracy: 0.9885\n",
            "Epoch 160/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0306 - accuracy: 0.9865\n",
            "Epoch 161/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0177 - accuracy: 0.9935\n",
            "Epoch 162/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 0.9925\n",
            "Epoch 163/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0300 - accuracy: 0.9870\n",
            "Epoch 164/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 0.9845\n",
            "Epoch 165/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0282 - accuracy: 0.9875\n",
            "Epoch 166/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0278 - accuracy: 0.9905\n",
            "Epoch 167/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 0.9870\n",
            "Epoch 168/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0356 - accuracy: 0.9820\n",
            "Epoch 169/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0218 - accuracy: 0.9900\n",
            "Epoch 170/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0156 - accuracy: 0.9910\n",
            "Epoch 171/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0245 - accuracy: 0.9905\n",
            "Epoch 172/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9890\n",
            "Epoch 173/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 0.9920\n",
            "Epoch 174/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0317 - accuracy: 0.9890\n",
            "Epoch 175/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 0.9920\n",
            "Epoch 176/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0300 - accuracy: 0.9915\n",
            "Epoch 177/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0222 - accuracy: 0.9885\n",
            "Epoch 178/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 0.9920\n",
            "Epoch 179/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9920\n",
            "Epoch 180/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 0.9905\n",
            "Epoch 181/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0255 - accuracy: 0.9915\n",
            "Epoch 182/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.9900\n",
            "Epoch 183/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 0.9900\n",
            "Epoch 184/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0181 - accuracy: 0.9900\n",
            "Epoch 185/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.9945\n",
            "Epoch 186/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0221 - accuracy: 0.9895\n",
            "Epoch 187/200\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 0.9920\n",
            "Epoch 188/200\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 0.0183 - accuracy: 0.9895\n",
            "Epoch 189/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0233 - accuracy: 0.9935\n",
            "Epoch 190/200\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.0331 - accuracy: 0.9865\n",
            "Epoch 191/200\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0243 - accuracy: 0.9885\n",
            "Epoch 192/200\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.0211 - accuracy: 0.9900\n",
            "Epoch 193/200\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.0213 - accuracy: 0.9865\n",
            "Epoch 194/200\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.0140 - accuracy: 0.9915\n",
            "Epoch 195/200\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.0293 - accuracy: 0.9905\n",
            "Epoch 196/200\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.0222 - accuracy: 0.9880\n",
            "Epoch 197/200\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 0.0210 - accuracy: 0.9895\n",
            "Epoch 198/200\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.0326 - accuracy: 0.9890\n",
            "Epoch 199/200\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 0.0139 - accuracy: 0.9925\n",
            "Epoch 200/200\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0130 - accuracy: 0.9910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model_vgg\n",
        "'''model_resnet.fit(features_resnet,Y_train,epochs=10,shuffle=True, verbose=2)\n",
        "\n",
        "# Save the trained model_vgg to a file so we can use it to make predictions later\n",
        "model_resnet.save(\"bird_feature_classifier_model_resnet.h5\")'''"
      ],
      "metadata": {
        "id": "6l4q7m2n4wEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZLgCYOnB1k-"
      },
      "source": [
        "from keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUqfBf9ECFBU"
      },
      "source": [
        "# Empty lists to hold the images and labels for each each image\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "# Load the test data set by looping over every image file\n",
        "for image_file in Path(\"Transfer-Learning/test_dataset\").glob(\"**/*.png\"):\n",
        "\n",
        "    # Load the current image file\n",
        "    image_data = load_img(image_file, target_size=(224,224,3))\n",
        "\n",
        "    # Convert the loaded image file to a numpy array\n",
        "    image_array = img_to_array(image_data)\n",
        "\n",
        "    # Add the current image to our list of test images\n",
        "    x_test.append(image_array)\n",
        "\n",
        "    # Add an expected label for this image. If it was a not_bird image, label it 0. If it was a bird, label it 1.\n",
        "    if \"not_bird\" in image_file.stem:\n",
        "        y_test.append(0)\n",
        "    else:\n",
        "        y_test.append(1)\n",
        "\n",
        "# Convert the list of test images to a numpy array\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "# Normalize test data set to 0-to-1 range\n",
        "x_test = imagenet_utils.preprocess_input(x_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "ygFBUl_cSOmT",
        "outputId": "289633b3-1160-4257-edac-70ba8767d091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPYbXZBMCSGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b637682-a92c-40d7-cd33-466387fa8cb1"
      },
      "source": [
        "# Load our trained classifier model_vgg\n",
        "model_vgg = load_model(\"bird_feature_classifier_model_vgg.h5\")\n",
        "\n",
        "# Extract features for each image (all in one pass)\n",
        "features_x = feature_extractor.predict(x_test)\n",
        "\n",
        "# Given the extracted features, make a final prediction using our own model_vgg\n",
        "predictions = model_vgg.predict(features_x)\n",
        "\n",
        "# If the model_vgg is more than 50% sure the object is a bird, call it a bird.\n",
        "# Otherwise, call it \"not a bird\".\n",
        "predictions = predictions > 0.5\n",
        "\n",
        "# Calculate how many mis-classifications the model_vgg makes\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
        "print(f\"True Positives: {tp}\")\n",
        "print(f\"True Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "\n",
        "# Calculate Precision and Recall for each class\n",
        "report = classification_report(y_test, predictions)\n",
        "print(report)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 3s 540ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "True Positives: 89\n",
            "True Negatives: 84\n",
            "False Positives: 16\n",
            "False Negatives: 11\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.84      0.86       100\n",
            "           1       0.85      0.89      0.87       100\n",
            "\n",
            "    accuracy                           0.86       200\n",
            "   macro avg       0.87      0.86      0.86       200\n",
            "weighted avg       0.87      0.86      0.86       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our trained classifier model_vgg\n",
        "#model_vgg = load_model(\"bird_feature_classifier_model_vgg.h5\")\n",
        "\n",
        "# Extract features for each image (all in one pass)\n",
        "features_x = feature_extractor_resnet.predict(x_test)\n",
        "\n",
        "# Given the extracted features, make a final prediction using our own model_vgg\n",
        "predictions = model_resnet.predict(features_x)\n",
        "\n",
        "# If the model_vgg is more than 50% sure the object is a bird, call it a bird.\n",
        "# Otherwise, call it \"not a bird\".\n",
        "predictions = predictions > 0.5\n",
        "\n",
        "# Calculate how many mis-classifications the model_vgg makes\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
        "print(f\"True Positives: {tp}\")\n",
        "print(f\"True Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "\n",
        "# Calculate Precision and Recall for each class\n",
        "report = classification_report(y_test, predictions)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "pvo5LC1fGdf3",
        "outputId": "343f6ecc-5211-4960-e3c1-18c211c0762d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 2s 263ms/step\n",
            "7/7 [==============================] - 0s 6ms/step\n",
            "True Positives: 92\n",
            "True Negatives: 83\n",
            "False Positives: 17\n",
            "False Negatives: 8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.83      0.87       100\n",
            "           1       0.84      0.92      0.88       100\n",
            "\n",
            "    accuracy                           0.88       200\n",
            "   macro avg       0.88      0.88      0.87       200\n",
            "weighted avg       0.88      0.88      0.87       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N97Mwb4Nt7Ow"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}